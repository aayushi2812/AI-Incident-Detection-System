{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b37981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge risk CSVs -> meta dataset -> train small ANN -> output incident probabilities and severity\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "# ----------------- USER CONFIG -----------------\n",
    "# Local file paths (change if your files are elsewhere)\n",
    "# NOTE: adjust these paths to your actual CSVs.\n",
    "DATA_RISK_CSV = \"/mnt/data/results/jm1_notebook_results/jm1_data_risk_scores.csv\"\n",
    "CODE_RISK_CSV = \"/mnt/data/code_risk_scores.csv\"   # replace with your code-risk CSV path\n",
    "# If you have a single combined file already, set one path and leave the other None.\n",
    "\n",
    "# Column names (auto-detection below will try to find id and label columns)\n",
    "ID_COL = None            # None => auto-detect common id column like 'id','file','module','name'\n",
    "DATA_RISK_COL = \"data_risk_score\"\n",
    "CODE_RISK_COL = \"code_risk_score\"\n",
    "INCIDENT_LABEL_COL = \"incident_label\"   # if you have ground-truth incident labels\n",
    "# If defects column exists in data_risk CSV it will be used as incident label if INCIDENT_LABEL_COL absent\n",
    "\n",
    "OUT_DIR = \"/mnt/data/meta_out\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "MODEL_OUT = os.path.join(OUT_DIR, \"meta_ann.h5\")\n",
    "SCALER_OUT = os.path.join(OUT_DIR, \"meta_scaler.pkl\")\n",
    "META_CSV_OUT = os.path.join(OUT_DIR, \"meta_dataset.csv\")\n",
    "PRED_CSV_OUT = os.path.join(OUT_DIR, \"meta_predictions.csv\")\n",
    "\n",
    "# Model hyperparams\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "LR = 1e-3\n",
    "PATIENCE = 8\n",
    "\n",
    "# Severity thresholds for classification (you can change these)\n",
    "THR_HIGH = 0.70\n",
    "THR_MED = 0.40   # medium is between 0.40 and 0.70\n",
    "# -------------------------------------------------\n",
    "\n",
    "# --------- Load CSVs (with sanity checks) ----------\n",
    "def safe_read(path):\n",
    "    if path is None:\n",
    "        return None\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "df_data = safe_read(DATA_RISK_CSV)\n",
    "df_code = safe_read(CODE_RISK_CSV)\n",
    "\n",
    "if df_data is None and df_code is None:\n",
    "    raise ValueError(\"At least one of DATA_RISK_CSV or CODE_RISK_CSV must be provided and exist.\")\n",
    "\n",
    "# Auto-detect id column if not set\n",
    "def find_id_col(dfs):\n",
    "    candidates = ['id','Id','ID','file','module','name']\n",
    "    for c in candidates:\n",
    "        ok = True\n",
    "        for df in dfs:\n",
    "            if df is None: continue\n",
    "            if c not in df.columns:\n",
    "                ok = False; break\n",
    "        if ok:\n",
    "            return c\n",
    "    # fallback: try intersection of column names\n",
    "    cols_sets = [set(df.columns) for df in dfs if df is not None]\n",
    "    inter = set.intersection(*cols_sets) if cols_sets else set()\n",
    "    for c in candidates:\n",
    "        if c in inter:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "ID_COL = ID_COL or find_id_col([df_data, df_code])\n",
    "if ID_COL is None:\n",
    "    # If no id, create an index-based id in both frames to merge by position\n",
    "    print(\"No common id column found — merging by index position and creating 'meta_id'.\")\n",
    "    if df_data is not None:\n",
    "        df_data = df_data.reset_index().rename(columns={'index': 'meta_id'})\n",
    "    if df_code is not None:\n",
    "        df_code = df_code.reset_index().rename(columns={'index': 'meta_id'})\n",
    "    ID_COL = 'meta_id'\n",
    "\n",
    "print(\"Using id column:\", ID_COL)\n",
    "\n",
    "# --- Prepare columns: ensure risk columns exist ---\n",
    "def ensure_col(df, col, default=None):\n",
    "    if df is None: return None\n",
    "    if col not in df.columns:\n",
    "        if default is None:\n",
    "            # create missing column with NaNs\n",
    "            df[col] = np.nan\n",
    "        else:\n",
    "            df[col] = default\n",
    "    return df\n",
    "\n",
    "if df_data is not None:\n",
    "    # if data risk saved under different name, try to find it\n",
    "    if DATA_RISK_COL not in df_data.columns:\n",
    "        possible = [c for c in df_data.columns if 'risk' in c.lower() or 'data' in c.lower()]\n",
    "        if possible:\n",
    "            print(\"Found possible data risk columns in data CSV:\", possible)\n",
    "            DATA_RISK_COL = possible[0]\n",
    "        else:\n",
    "            df_data[DATA_RISK_COL] = np.nan\n",
    "\n",
    "if df_code is not None:\n",
    "    if CODE_RISK_COL not in df_code.columns:\n",
    "        possible = [c for c in df_code.columns if 'risk' in c.lower() or 'code' in c.lower()]\n",
    "        if possible:\n",
    "            print(\"Found possible code risk columns in code CSV:\", possible)\n",
    "            CODE_RISK_COL = possible[0]\n",
    "        else:\n",
    "            df_code[CODE_RISK_COL] = np.nan\n",
    "\n",
    "# --- Merge dataframes on ID_COL ---\n",
    "if df_data is None:\n",
    "    meta = df_code[[ID_COL, CODE_RISK_COL]].copy()\n",
    "elif df_code is None:\n",
    "    meta = df_data[[ID_COL, DATA_RISK_COL]].copy()\n",
    "else:\n",
    "    meta = pd.merge(df_data[[ID_COL, DATA_RISK_COL]], df_code[[ID_COL, CODE_RISK_COL]], on=ID_COL, how='outer')\n",
    "\n",
    "# If defects column exists in data_risk CSV, use it as incident label if INCIDENT_LABEL_COL not present\n",
    "if INCIDENT_LABEL_COL not in meta.columns:\n",
    "    if df_data is not None and 'defects' in df_data.columns:\n",
    "        meta = meta.merge(df_data[[ID_COL, 'defects']], on=ID_COL, how='left')\n",
    "        meta.rename(columns={'defects': INCIDENT_LABEL_COL}, inplace=True)\n",
    "    elif df_code is not None and 'defects' in df_code.columns:\n",
    "        meta = meta.merge(df_code[[ID_COL, 'defects']], on=ID_COL, how='left')\n",
    "        meta.rename(columns={'defects': INCIDENT_LABEL_COL}, inplace=True)\n",
    "\n",
    "# Keep only relevant columns\n",
    "cols_keep = [ID_COL, DATA_RISK_COL, CODE_RISK_COL]\n",
    "if INCIDENT_LABEL_COL in meta.columns:\n",
    "    cols_keep.append(INCIDENT_LABEL_COL)\n",
    "meta = meta[cols_keep]\n",
    "\n",
    "# Fill missing risk scores sensibly (impute 0 for missing code_risk if not available)\n",
    "meta[DATA_RISK_COL] = meta[DATA_RISK_COL].fillna(0.0)\n",
    "meta[CODE_RISK_COL] = meta[CODE_RISK_COL].fillna(0.0)\n",
    "\n",
    "# Save meta dataset (before training)\n",
    "meta.to_csv(META_CSV_OUT, index=False)\n",
    "print(\"Saved meta dataset to\", META_CSV_OUT)\n",
    "\n",
    "# ----------------- Prepare training data -----------------\n",
    "# If no incident_label present, create a synthetic label using a heuristic (optional)\n",
    "if INCIDENT_LABEL_COL not in meta.columns or meta[INCIDENT_LABEL_COL].isna().all():\n",
    "    print(\"No incident_label found — creating synthetic labels using heuristic.\")\n",
    "    # Simple heuristic: incident if both risks high\n",
    "    meta[INCIDENT_LABEL_COL] = ((meta[DATA_RISK_COL] >= 0.7) & (meta[CODE_RISK_COL] >= 0.6)).astype(int)\n",
    "    synthetic_label_used = True\n",
    "else:\n",
    "    # ensure int 0/1\n",
    "    meta[INCIDENT_LABEL_COL] = meta[INCIDENT_LABEL_COL].fillna(0).astype(int)\n",
    "    synthetic_label_used = False\n",
    "\n",
    "# Features and target\n",
    "feature_cols = [DATA_RISK_COL, CODE_RISK_COL]\n",
    "X = meta[feature_cols].values.astype(float)\n",
    "y = meta[INCIDENT_LABEL_COL].values.astype(int)\n",
    "\n",
    "# Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_val_s = scaler.transform(X_val)\n",
    "joblib.dump(scaler, SCALER_OUT)\n",
    "print(\"Saved scaler to\", SCALER_OUT)\n",
    "\n",
    "# ----------------- Build & train small ANN -----------------\n",
    "def build_meta_model(input_dim):\n",
    "    inp = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(16, activation='relu')(inp)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(8, activation='relu')(x)\n",
    "    out = layers.Dense(1, activation='sigmoid')(x)\n",
    "    m = models.Model(inp, out)\n",
    "    m.compile(optimizer=optimizers.Adam(learning_rate=LR), loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return m\n",
    "\n",
    "model = build_meta_model(X_train_s.shape[1])\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_s, y_train, validation_data=(X_val_s, y_val),\n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=1)\n",
    "\n",
    "# Save model\n",
    "model.save(MODEL_OUT)\n",
    "print(\"Saved meta model to\", MODEL_OUT)\n",
    "\n",
    "# --------------- Inference on full meta dataset ----------------\n",
    "X_all_s = scaler.transform(meta[feature_cols].values.astype(float))\n",
    "probs = model.predict(X_all_s).squeeze()\n",
    "meta['incident_probability'] = probs\n",
    "\n",
    "# Severity classification\n",
    "def severity_from_prob(p):\n",
    "    if p >= THR_HIGH:\n",
    "        return \"High\"\n",
    "    if p >= THR_MED:\n",
    "        return \"Medium\"\n",
    "    return \"Low\"\n",
    "\n",
    "meta['incident_severity'] = meta['incident_probability'].apply(severity_from_prob)\n",
    "\n",
    "# Save predictions\n",
    "meta.to_csv(PRED_CSV_OUT, index=False)\n",
    "print(\"Saved predictions to\", PRED_CSV_OUT)\n",
    "\n",
    "# --------------- Evaluation (if we have real labels) ----------------\n",
    "if not synthetic_label_used:\n",
    "    y_true_full = meta[INCIDENT_LABEL_COL].values\n",
    "    y_score_full = meta['incident_probability'].values\n",
    "    try:\n",
    "        roc = roc_auc_score(y_true_full, y_score_full)\n",
    "        prec, rec, _ = precision_recall_curve(y_true_full, y_score_full)\n",
    "        pr_auc = auc(rec, prec)\n",
    "        print(f\"ROC-AUC: {roc:.4f}, PR-AUC: {pr_auc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"Evaluation failed:\", e)\n",
    "else:\n",
    "    print(\"Synthetic labels were used; evaluation metrics may be misleading.\")\n",
    "\n",
    "# --------------- Quick summary output ----------------\n",
    "print(\"\\nTop 10 predicted incidents (by prob):\")\n",
    "print(meta.sort_values('incident_probability', ascending=False).head(10)[[ID_COL, DATA_RISK_COL, CODE_RISK_COL, 'incident_probability', 'incident_severity']])\n",
    "\n",
    "# End\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
